{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "import zipfile   # <-- add this line\n",
        "\n",
        "# Unzip your uploaded file\n",
        "with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"langData\")  # Extract into folder \"langData\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "lGK-YhbUV4Cv",
        "outputId": "6ed543f5-bb54-4283-d0d3-403b97f0dda3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-535ac05e-3369-45e9-9a66-cf4e396acd0d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-535ac05e-3369-45e9-9a66-cf4e396acd0d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset.zip to dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj6ukb5LUgFj"
      },
      "source": [
        "# Import Libraries and Set Up\n",
        "\n",
        "---\n",
        "\n",
        "EnvironmentThis cell imports necessary\n",
        "\n",
        "---\n",
        "\n",
        "libraries, sets random seeds for reproducibility, and configures the device (CPU/GPU) for PyTorch operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J6PjOdaUgFp",
        "outputId": "99b44a5d-b3c3-4186-d593-cc5897f95706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Install required packages (run in Kaggle)\n",
        "!pip install sentencepiece\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Set random seeds\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-JIou48UgFu"
      },
      "source": [
        "# Data Preprocessing ClassThe\n",
        "\n",
        "---\n",
        "\n",
        "`UrduRomanDataProcessor` class handles loading, cleaning, and splitting Urdu-Roman text pairs from the `urdu_ghazals_rekhta` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0zS-si7bUgFw"
      },
      "outputs": [],
      "source": [
        "class UrduRomanDataProcessor:\n",
        "    \"\"\"Data processor for Urdu-Roman translation pairs from urdu_ghazals_rekhta dataset\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: str):\n",
        "        self.dataset_path = Path(\"/content/langData/dataset - Copy\")\n",
        "        self.urdu_texts = []\n",
        "        self.roman_texts = []\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load data from the urdu_ghazals_rekhta dataset structure\"\"\"\n",
        "        print(\"Loading data from urdu_ghazals_rekhta dataset...\")\n",
        "\n",
        "        if not self.dataset_path.exists():\n",
        "            raise FileNotFoundError(f\"Dataset path {self.dataset_path} not found\")\n",
        "\n",
        "        # Dataset structure: poets -> [ur, en, hi] -> files (no extensions)\n",
        "        for poet_dir in self.dataset_path.iterdir():\n",
        "            if not poet_dir.is_dir() or poet_dir.name.startswith('.'):\n",
        "                continue\n",
        "\n",
        "            urdu_dir = poet_dir / 'ur'\n",
        "            english_dir = poet_dir / 'en'  # This contains Roman Urdu transliteration\n",
        "\n",
        "            if not (urdu_dir.exists() and english_dir.exists()):\n",
        "                continue\n",
        "\n",
        "            # Get all Urdu files (no extension filter needed)\n",
        "            urdu_files = [f for f in urdu_dir.iterdir() if f.is_file() and not f.name.startswith('.')]\n",
        "\n",
        "            for urdu_file in urdu_files:\n",
        "                english_file = english_dir / urdu_file.name\n",
        "\n",
        "                if english_file.exists() and english_file.is_file():\n",
        "                    try:\n",
        "                        # Read Urdu text\n",
        "                        with open(urdu_file, 'r', encoding='utf-8') as f:\n",
        "                            urdu_content = f.read().strip()\n",
        "\n",
        "                        # Read Roman Urdu text\n",
        "                        with open(english_file, 'r', encoding='utf-8') as f:\n",
        "                            roman_content = f.read().strip()\n",
        "\n",
        "                        # Split by lines to get verse pairs\n",
        "                        urdu_lines = [line.strip() for line in urdu_content.split('\\n') if line.strip()]\n",
        "                        roman_lines = [line.strip() for line in roman_content.split('\\n') if line.strip()]\n",
        "\n",
        "                        # Pair up lines (verses)\n",
        "                        for urdu_line, roman_line in zip(urdu_lines, roman_lines):\n",
        "                            if urdu_line and roman_line:\n",
        "                                self.urdu_texts.append(urdu_line)\n",
        "                                self.roman_texts.append(roman_line)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {urdu_file.name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        print(f\"Loaded {len(self.urdu_texts)} text pairs\")\n",
        "\n",
        "        if len(self.urdu_texts) == 0:\n",
        "            raise ValueError(\"No data loaded. Check dataset structure and paths.\")\n",
        "\n",
        "    def clean_text(self, text: str, is_urdu: bool = True) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Unicode normalization\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        if is_urdu:\n",
        "            # Keep Urdu characters and basic punctuation\n",
        "            text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n",
        "        else:\n",
        "            # Convert to lowercase and keep Roman characters\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'[^a-z0-9\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def preprocess_data(self, min_words=3, max_words=50):\n",
        "        \"\"\"Clean and filter the data\"\"\"\n",
        "        print(\"Preprocessing and filtering data...\")\n",
        "\n",
        "        cleaned_urdu = []\n",
        "        cleaned_roman = []\n",
        "\n",
        "        for urdu, roman in zip(self.urdu_texts, self.roman_texts):\n",
        "            # Clean texts\n",
        "            clean_urdu = self.clean_text(urdu, is_urdu=True)\n",
        "            clean_roman = self.clean_text(roman, is_urdu=False)\n",
        "\n",
        "            # Filter by length\n",
        "            urdu_words = len(clean_urdu.split())\n",
        "            roman_words = len(clean_roman.split())\n",
        "\n",
        "            if (min_words <= urdu_words <= max_words and\n",
        "                min_words <= roman_words <= max_words and\n",
        "                clean_urdu and clean_roman):\n",
        "                cleaned_urdu.append(clean_urdu)\n",
        "                cleaned_roman.append(clean_roman)\n",
        "\n",
        "        self.urdu_texts = cleaned_urdu\n",
        "        self.roman_texts = cleaned_roman\n",
        "\n",
        "        print(f\"After preprocessing: {len(self.urdu_texts)} pairs\")\n",
        "\n",
        "        if len(self.urdu_texts) < 100:\n",
        "            print(\"Warning: Very few text pairs available. Consider relaxing filtering criteria.\")\n",
        "\n",
        "    def split_data(self, test_size=0.25, val_size=0.25, random_state=42):\n",
        "        \"\"\"Split data into train/val/test sets (50/25/25 as required)\"\"\"\n",
        "        # First split: separate test set (25%)\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            self.urdu_texts, self.roman_texts,\n",
        "            test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Second split: divide remaining into train/val\n",
        "        val_adjusted = val_size / (1 - test_size)  # 0.25 / 0.75 = 0.333\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_adjusted, random_state=random_state\n",
        "        )\n",
        "\n",
        "        print(f\"Data split - Train: {len(X_train)} ({len(X_train)/len(self.urdu_texts)*100:.1f}%), \"\n",
        "              f\"Val: {len(X_val)} ({len(X_val)/len(self.urdu_texts)*100:.1f}%), \"\n",
        "              f\"Test: {len(X_test)} ({len(X_test)/len(self.urdu_texts)*100:.1f}%)\")\n",
        "\n",
        "        return {\n",
        "            'train': {'urdu': X_train, 'roman': y_train},\n",
        "            'val': {'urdu': X_val, 'roman': y_val},\n",
        "            'test': {'urdu': X_test, 'roman': y_test}\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnYSygAJUgFz"
      },
      "source": [
        "# Tokenizer CreationThis cell defines a\n",
        "\n",
        "---\n",
        "\n",
        "function to create and train SentencePiece tokenizers for Urdu and Roman texts, handling vocabulary size estimation and temporary file management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZDbfE9DLUgF0"
      },
      "outputs": [],
      "source": [
        "def create_tokenizers(urdu_texts, roman_texts, vocab_size=8000):\n",
        "    \"\"\"Create SentencePiece tokenizers for Urdu and Roman text\"\"\"\n",
        "    import tempfile\n",
        "    import os\n",
        "\n",
        "    # Create temporary files for training data\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n",
        "        urdu_temp_file = f.name\n",
        "        for text in urdu_texts:\n",
        "            f.write(text + '\\n')\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n",
        "        roman_temp_file = f.name\n",
        "        for text in roman_texts:\n",
        "            f.write(text + '\\n')\n",
        "\n",
        "    try:\n",
        "        # Train Urdu tokenizer\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=urdu_temp_file,\n",
        "            model_prefix='urdu_tokenizer',\n",
        "            vocab_size=vocab_size,\n",
        "            character_coverage=1.0,\n",
        "            model_type='bpe',\n",
        "            pad_id=0,\n",
        "            unk_id=1,\n",
        "            bos_id=2,\n",
        "            eos_id=3\n",
        "        )\n",
        "\n",
        "        # Train Roman tokenizer\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=roman_temp_file,\n",
        "            model_prefix='roman_tokenizer',\n",
        "            vocab_size=vocab_size,\n",
        "            character_coverage=1.0,\n",
        "            model_type='bpe',\n",
        "            pad_id=0,\n",
        "            unk_id=1,\n",
        "            bos_id=2,\n",
        "            eos_id=3\n",
        "        )\n",
        "\n",
        "        # Load tokenizers\n",
        "        urdu_tokenizer = spm.SentencePieceProcessor()\n",
        "        urdu_tokenizer.load('urdu_tokenizer.model')\n",
        "\n",
        "        roman_tokenizer = spm.SentencePieceProcessor()\n",
        "        roman_tokenizer.load('roman_tokenizer.model')\n",
        "\n",
        "        print(f\"Urdu tokenizer vocabulary size: {urdu_tokenizer.get_piece_size()}\")\n",
        "        print(f\"Roman tokenizer vocabulary size: {roman_tokenizer.get_piece_size()}\")\n",
        "\n",
        "        return urdu_tokenizer, roman_tokenizer\n",
        "\n",
        "    finally:\n",
        "        # Clean up temporary files\n",
        "        try:\n",
        "            os.unlink(urdu_temp_file)\n",
        "            os.unlink(roman_temp_file)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwn7REWAUgF2"
      },
      "source": [
        "# Dataset and DataLoaderThis cell defines\n",
        "\n",
        "---\n",
        "\n",
        "the `TranslationDataset` class and a `collate_fn` function for creating datasets and data loaders with proper tokenization and padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nE47akFbUgF3"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Dataset for translation pairs\"\"\"\n",
        "\n",
        "    def __init__(self, urdu_texts, roman_texts, urdu_tokenizer, roman_tokenizer, max_length=50):\n",
        "        self.urdu_texts = urdu_texts\n",
        "        self.roman_texts = roman_texts\n",
        "        self.urdu_tokenizer = urdu_tokenizer\n",
        "        self.roman_tokenizer = roman_tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urdu_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        urdu_text = self.urdu_texts[idx]\n",
        "        roman_text = self.roman_texts[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        urdu_tokens = self.urdu_tokenizer.encode(urdu_text, out_type=int)\n",
        "        roman_tokens = self.roman_tokenizer.encode(roman_text, out_type=int)\n",
        "\n",
        "        # Ensure BOS and EOS in target and respect max_length\n",
        "        roman_tokens = roman_tokens[:max(0, self.max_length - 2)]\n",
        "        roman_tokens = [2] + roman_tokens + [3]\n",
        "\n",
        "        # Truncate if necessary\n",
        "        urdu_tokens = urdu_tokens[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            'urdu': torch.tensor(urdu_tokens, dtype=torch.long),\n",
        "            'roman': torch.tensor(roman_tokens, dtype=torch.long),\n",
        "            'urdu_text': urdu_text,\n",
        "            'roman_text': roman_text\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function with padding\"\"\"\n",
        "    urdu_seqs = [item['urdu'] for item in batch]\n",
        "    roman_seqs = [item['roman'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    urdu_padded = nn.utils.rnn.pad_sequence(urdu_seqs, batch_first=True, padding_value=0)\n",
        "    roman_padded = nn.utils.rnn.pad_sequence(roman_seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'urdu': urdu_padded,\n",
        "        'roman': roman_padded,\n",
        "        'urdu_texts': [item['urdu_text'] for item in batch],\n",
        "        'roman_texts': [item['roman_text'] for item in batch]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0XxuOchUgF5"
      },
      "source": [
        "# Model ArchitectureThis cell defines the\n",
        "\n",
        "---\n",
        "\n",
        "BiLSTM-based encoder-decoder model with attention, including `BiLSTMEncoder`, `Attention`, `LSTMDecoder`, and `Seq2SeqModel` classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yEx-2zxwUgF7"
      },
      "outputs": [],
      "source": [
        "class BiLSTMEncoder(nn.Module):\n",
        "    \"\"\"BiLSTM Encoder (2 layers as required)\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.1):\n",
        "        super(BiLSTMEncoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        # Pack padded sequence for efficiency if lengths provided\n",
        "        if lengths is not None:\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                embedded, lengths, batch_first=True, enforce_sorted=False\n",
        "            )\n",
        "            outputs, (hidden, cell) = self.lstm(packed)\n",
        "            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        else:\n",
        "            outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism with masking support\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # hidden: (batch_size, hidden_dim)\n",
        "        # encoder_outputs: (batch_size, seq_len, hidden_dim*2)\n",
        "        # mask: (batch_size, seq_len) - 1 for valid positions, 0 for padding\n",
        "\n",
        "        batch_size, seq_len, _ = encoder_outputs.size()\n",
        "\n",
        "        # Repeat hidden for all encoder positions\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # Compute attention energies\n",
        "        energy = torch.cat([hidden, encoder_outputs], dim=2)\n",
        "        energy = torch.tanh(self.attn(energy))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = F.softmax(attention, dim=1)\n",
        "\n",
        "        # Apply attention to encoder outputs\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder with attention (4 layers as required)\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=4, dropout=0.1):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim + hidden_dim * 2, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.output_projection = nn.Linear(hidden_dim + hidden_dim * 2, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, hidden_state, encoder_outputs, encoder_mask=None):\n",
        "        # x: (batch_size, 1) - single time step\n",
        "        # Ensure x has correct shape\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        # Get context from attention\n",
        "        context, attention_weights = self.attention(\n",
        "            hidden_state[0][-1], encoder_outputs, encoder_mask\n",
        "        )\n",
        "\n",
        "        # Concatenate embedding with context\n",
        "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
        "\n",
        "        # LSTM forward\n",
        "        output, hidden_state = self.lstm(lstm_input, hidden_state)\n",
        "\n",
        "        # Final output projection\n",
        "        final_output = torch.cat([output, context.unsqueeze(1)], dim=2)\n",
        "        predictions = self.output_projection(final_output)\n",
        "\n",
        "        return predictions, hidden_state, attention_weights\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    \"\"\"Seq2Seq model with BiLSTM encoder and LSTM decoder\"\"\"\n",
        "\n",
        "    def __init__(self, urdu_vocab_size, roman_vocab_size, embed_dim=128, hidden_dim=256,\n",
        "                 encoder_layers=2, decoder_layers=4, dropout=0.1):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "\n",
        "        self.encoder = BiLSTMEncoder(urdu_vocab_size, embed_dim, hidden_dim, encoder_layers, dropout)\n",
        "        self.decoder = LSTMDecoder(roman_vocab_size, embed_dim, hidden_dim, decoder_layers, dropout)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.decoder_layers = decoder_layers\n",
        "\n",
        "        # Bridge to convert bidirectional encoder hidden to decoder hidden\n",
        "        self.bridge_h = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.bridge_c = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, urdu_seq, roman_seq=None, teacher_forcing_ratio=0.5):\n",
        "        batch_size = urdu_seq.size(0)\n",
        "        device = urdu_seq.device\n",
        "\n",
        "        # Create encoder mask\n",
        "        encoder_mask = (urdu_seq != 0).float()\n",
        "        encoder_lengths = encoder_mask.sum(dim=1).cpu()\n",
        "\n",
        "        # Encode\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(urdu_seq, encoder_lengths)\n",
        "\n",
        "        # Convert bidirectional encoder states to decoder states\n",
        "        encoder_hidden = encoder_hidden.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n",
        "        encoder_cell = encoder_cell.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n",
        "\n",
        "        # Concatenate forward and backward states\n",
        "        last_hidden = torch.cat([encoder_hidden[-1, 0], encoder_hidden[-1, 1]], dim=1)\n",
        "        last_cell = torch.cat([encoder_cell[-1, 0], encoder_cell[-1, 1]], dim=1)\n",
        "\n",
        "        # Bridge to decoder dimensions\n",
        "        decoder_hidden = self.bridge_h(last_hidden).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n",
        "        decoder_cell = self.bridge_c(last_cell).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n",
        "\n",
        "        if roman_seq is not None:  # Training mode\n",
        "            max_length = roman_seq.size(1)\n",
        "            outputs = []\n",
        "            input_token = roman_seq[:, 0:1]  # SOS保证\n",
        "            hidden_state = (decoder_hidden, decoder_cell)\n",
        "\n",
        "            for t in range(max_length - 1):\n",
        "                output, hidden_state, _ = self.decoder(\n",
        "                    input_token, hidden_state, encoder_outputs, encoder_mask\n",
        "                )\n",
        "                outputs.append(output)\n",
        "\n",
        "                # Teacher forcing\n",
        "                if random.random() < teacher_forcing_ratio:\n",
        "                    input_token = roman_seq[:, t+1:t+2]\n",
        "                else:\n",
        "                    input_token = output.argmax(dim=-1)\n",
        "\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "        else:  # Inference mode with beam search\n",
        "            return self.beam_search_decode(\n",
        "                encoder_outputs, encoder_mask, decoder_hidden, decoder_cell,\n",
        "                beam_size=3, max_length=50\n",
        "            )\n",
        "\n",
        "    def beam_search_decode(self, encoder_outputs, encoder_mask, decoder_hidden,\n",
        "                          decoder_cell, beam_size=3, max_length=50):\n",
        "        \"\"\"Beam search decoding to avoid repetition\"\"\"\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        device = encoder_outputs.device\n",
        "\n",
        "        # For simplicity, handle batch_size=1 (extend for batches if needed)\n",
        "        if batch_size > 1:\n",
        "            # Fall back to greedy for batch processing\n",
        "            return self.greedy_decode(encoder_outputs, encoder_mask, decoder_hidden,\n",
        "                                    decoder_cell, max_length)\n",
        "\n",
        "        # Initialize beams\n",
        "        beams = [([], 0.0, (decoder_hidden, decoder_cell))]  # (sequence, score, hidden_state)\n",
        "        completed = []\n",
        "\n",
        "        # Start token\n",
        "        start_token = torch.tensor([[2]], dtype=torch.long, device=device)  # SOS\n",
        "\n",
        "        for step in range(max_length):\n",
        "            candidates = []\n",
        "\n",
        "            for seq, score, hidden_state in beams:\n",
        "                if len(seq) > 0 and seq[-1] == 3:  # EOS token\n",
        "                    completed.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Get input token\n",
        "                if len(seq) == 0:\n",
        "                    input_token = start_token\n",
        "                else:\n",
        "                    input_token = torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n",
        "\n",
        "                # Decode one step\n",
        "                output, new_hidden, _ = self.decoder(\n",
        "                    input_token, hidden_state, encoder_outputs, encoder_mask\n",
        "                )\n",
        "\n",
        "                # Get top k tokens\n",
        "                log_probs = F.log_softmax(output.squeeze(1), dim=-1)\n",
        "                top_k_scores, top_k_tokens = log_probs.topk(beam_size)\n",
        "\n",
        "                for k in range(beam_size):\n",
        "                    token = top_k_tokens[0, k].item()\n",
        "                    token_score = top_k_scores[0, k].item()\n",
        "\n",
        "                    # Apply repetition penalty\n",
        "                    if len(seq) > 0 and token == seq[-1]:\n",
        "                        token_score -= 2.0  # Penalty for immediate repetition\n",
        "\n",
        "                    # Check for pattern repetition\n",
        "                    if len(seq) > 3:\n",
        "                        recent = seq[-3:]\n",
        "                        if recent.count(token) > 1:\n",
        "                            token_score -= 3.0  # Higher penalty for patterns\n",
        "\n",
        "                    new_seq = seq + [token]\n",
        "                    new_score = score + token_score\n",
        "\n",
        "                    candidates.append((new_seq, new_score, new_hidden))\n",
        "\n",
        "            # Select top beams\n",
        "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = candidates[:beam_size]\n",
        "\n",
        "            # Early stopping if all beams are completed\n",
        "            if len(beams) == 0:\n",
        "                break\n",
        "\n",
        "        # Add remaining beams to completed\n",
        "        completed.extend(beams)\n",
        "\n",
        "        # Select best sequence\n",
        "        if completed:\n",
        "            best_seq = max(completed, key=lambda x: x[1] / len(x[0]))[0]  # Length normalization\n",
        "        else:\n",
        "            best_seq = beams[0][0] if beams else []\n",
        "\n",
        "        # Convert to tensor\n",
        "        if best_seq:\n",
        "            output_tensor = torch.tensor([best_seq], dtype=torch.long, device=device)\n",
        "        else:\n",
        "            output_tensor = torch.tensor([[3]], dtype=torch.long, device=device)  # EOS only\n",
        "\n",
        "        # Create dummy output for compatibility\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "        output_probs = torch.zeros(1, len(best_seq), vocab_size, device=device)\n",
        "        for i, token in enumerate(best_seq):\n",
        "            output_probs[0, i, token] = 1.0\n",
        "\n",
        "        return output_probs\n",
        "\n",
        "    def greedy_decode(self, encoder_outputs, encoder_mask, decoder_hidden,\n",
        "                     decoder_cell, max_length=50):\n",
        "        \"\"\"Greedy decoding with repetition penalty\"\"\"\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        device = encoder_outputs.device\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        outputs = []\n",
        "        input_token = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)  # SOS\n",
        "        hidden_state = (decoder_hidden, decoder_cell)\n",
        "\n",
        "        # Track recent tokens for repetition penalty\n",
        "        recent_tokens = []\n",
        "\n",
        "        for step in range(max_length):\n",
        "            output, hidden_state, _ = self.decoder(\n",
        "                input_token, hidden_state, encoder_outputs, encoder_mask\n",
        "            )\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if len(recent_tokens) > 0:\n",
        "                for recent_token in recent_tokens[-3:]:  # Penalize last 3 tokens\n",
        "                    output[0, 0, recent_token] -= 5.0\n",
        "\n",
        "            outputs.append(output)\n",
        "\n",
        "            # Get next token\n",
        "            input_token = output.argmax(dim=-1)\n",
        "            token_id = input_token.item() if batch_size == 1 else input_token[0].item()\n",
        "\n",
        "            recent_tokens.append(token_id)\n",
        "\n",
        "            # Stop if EOS token\n",
        "            if token_id == 3:\n",
        "                break\n",
        "\n",
        "        return torch.cat(outputs, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rynqe1jcUgGA"
      },
      "source": [
        "# Evaluation MetricsThis cell implements\n",
        "\n",
        "---\n",
        "\n",
        "evaluation metrics including perplexity, Character Error Rate (CER), BLEU score, and accuracy calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mQJfV2sPUgGB"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(model, data_loader, criterion, device):\n",
        "    \"\"\"Calculate perplexity\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            urdu_seq = batch['urdu'].to(device)\n",
        "            roman_seq = batch['roman'].to(device)\n",
        "\n",
        "            decoder_target = roman_seq[:, 1:]\n",
        "            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio=0.0)\n",
        "\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n",
        "            non_pad_tokens = (decoder_target != 0).sum().item()\n",
        "\n",
        "            total_loss += loss.item() * non_pad_tokens\n",
        "            total_tokens += non_pad_tokens\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return math.exp(avg_loss)\n",
        "\n",
        "def calculate_cer(predictions, targets, tokenizer):\n",
        "    \"\"\"Calculate Character Error Rate\"\"\"\n",
        "    def edit_distance_cer(s1, s2):\n",
        "        if len(s1) < len(s2):\n",
        "            return edit_distance_cer(s2, s1)\n",
        "\n",
        "        if len(s2) == 0:\n",
        "            return len(s1)\n",
        "\n",
        "        previous_row = range(len(s2) + 1)\n",
        "        for i, c1 in enumerate(s1):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(s2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "\n",
        "        return previous_row[-1]\n",
        "\n",
        "    total_chars = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        # Convert to lists and remove special tokens\n",
        "        if hasattr(pred, 'tolist'):\n",
        "            pred_tokens = pred.tolist()\n",
        "        else:\n",
        "            pred_tokens = list(pred)\n",
        "\n",
        "        if hasattr(target, 'tolist'):\n",
        "            target_tokens = target.tolist()\n",
        "        else:\n",
        "            target_tokens = list(target)\n",
        "\n",
        "        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n",
        "        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n",
        "\n",
        "        if len(pred_clean) > 0 and len(target_clean) > 0:\n",
        "            pred_text = tokenizer.decode(pred_clean)\n",
        "            target_text = tokenizer.decode(target_clean)\n",
        "\n",
        "            errors = edit_distance_cer(pred_text, target_text)\n",
        "            total_errors += errors\n",
        "            total_chars += len(target_text)\n",
        "\n",
        "    return total_errors / total_chars if total_chars > 0 else 1.0\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    \"\"\"Calculate edit distance (Levenshtein distance) between two strings\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return edit_distance(s2, s1)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]\n",
        "\n",
        "def calculate_bleu_score(predictions, targets, tokenizer):\n",
        "    \"\"\"Calculate BLEU score properly\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    def get_ngrams(tokens, n):\n",
        "        if len(tokens) < n:\n",
        "            return []\n",
        "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "    def calculate_bleu(pred_tokens, target_tokens, max_n=4):\n",
        "        if len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precisions = []\n",
        "        for n in range(1, min(max_n + 1, len(pred_tokens) + 1)):\n",
        "            pred_ngrams = Counter(get_ngrams(pred_tokens, n))\n",
        "            target_ngrams = Counter(get_ngrams(target_tokens, n))\n",
        "\n",
        "            if len(pred_ngrams) == 0:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            matches = sum((pred_ngrams & target_ngrams).values())\n",
        "            total = sum(pred_ngrams.values())\n",
        "            precision = matches / total if total > 0 else 0.0\n",
        "            precisions.append(precision)\n",
        "\n",
        "        # Brevity penalty\n",
        "        if len(pred_tokens) == 0:\n",
        "            bp = 0.0\n",
        "        elif len(pred_tokens) < len(target_tokens):\n",
        "            bp = math.exp(1 - len(target_tokens) / len(pred_tokens))\n",
        "        else:\n",
        "            bp = 1.0\n",
        "\n",
        "        # Geometric mean of precisions\n",
        "        if precisions and all(p > 0 for p in precisions):\n",
        "            geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
        "            score = bp * geo_mean\n",
        "        else:\n",
        "            score = 0.0\n",
        "\n",
        "        return score\n",
        "\n",
        "    total_score = 0.0\n",
        "    count = 0\n",
        "    total_errors = 0\n",
        "    total_chars = 0\n",
        "\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        # Handle tensors\n",
        "        if hasattr(pred, 'cpu'):\n",
        "            pred = pred.cpu()\n",
        "        if hasattr(target, 'cpu'):\n",
        "            target = target.cpu()\n",
        "\n",
        "        # Convert to lists\n",
        "        if hasattr(pred, 'tolist'):\n",
        "            pred_tokens = pred.tolist()\n",
        "        else:\n",
        "            pred_tokens = list(pred)\n",
        "\n",
        "        if hasattr(target, 'tolist'):\n",
        "            target_tokens = target.tolist()\n",
        "        else:\n",
        "            target_tokens = list(target)\n",
        "\n",
        "        # Remove special tokens\n",
        "        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n",
        "        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n",
        "\n",
        "        if len(pred_clean) > 0 and len(target_clean) > 0:\n",
        "            # Calculate BLEU score\n",
        "            bleu = calculate_bleu(pred_clean, target_clean)\n",
        "            total_score += bleu\n",
        "            count += 1\n",
        "\n",
        "            # Calculate CER (Character Error Rate)\n",
        "            pred_text = tokenizer.decode(pred_clean)\n",
        "            target_text = tokenizer.decode(target_clean)\n",
        "\n",
        "            if len(pred_text) > 0 and len(target_text) > 0:\n",
        "                errors = edit_distance(pred_text, target_text)\n",
        "                total_errors += errors\n",
        "                total_chars += len(target_text)\n",
        "\n",
        "    # Return BLEU score (not CER)\n",
        "    return total_score / count if count > 0 else 0.0\n",
        "\n",
        "def calculate_accuracy(predictions, targets, tokenizer):\n",
        "    \"\"\"Calculate token-level and sequence-level accuracy\"\"\"\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    total_sequences = 0\n",
        "    correct_sequences = 0\n",
        "\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        # Convert to lists and remove special tokens\n",
        "        if hasattr(pred, 'tolist'):\n",
        "            pred_tokens = pred.tolist()\n",
        "        else:\n",
        "            pred_tokens = list(pred)\n",
        "\n",
        "        if hasattr(target, 'tolist'):\n",
        "            target_tokens = target.tolist()\n",
        "        else:\n",
        "            target_tokens = list(target)\n",
        "\n",
        "        # Remove special tokens (padding, start, end, unknown)\n",
        "        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n",
        "        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n",
        "\n",
        "        if len(pred_clean) > 0 and len(target_clean) > 0:\n",
        "            # Token-level accuracy\n",
        "            min_len = min(len(pred_clean), len(target_clean))\n",
        "            max_len = max(len(pred_clean), len(target_clean))\n",
        "\n",
        "            # Count matching tokens up to the minimum length\n",
        "            matches = sum(1 for i in range(min_len) if pred_clean[i] == target_clean[i])\n",
        "            correct_tokens += matches\n",
        "            total_tokens += max_len  # Use max length to penalize length differences\n",
        "\n",
        "            # Sequence-level accuracy (exact match)\n",
        "            if pred_clean == target_clean:\n",
        "                correct_sequences += 1\n",
        "            total_sequences += 1\n",
        "\n",
        "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
        "    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'token_accuracy': token_accuracy,\n",
        "        'sequence_accuracy': sequence_accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLhlofi8UgGC"
      },
      "source": [
        "# Training and Evaluation FunctionsThis\n",
        "\n",
        "---\n",
        "\n",
        "cell contains functions for training an epoch, evaluating the model, and translating individual texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YSElTI7_UgGD"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        urdu_seq = batch['urdu'].to(device)\n",
        "        roman_seq = batch['roman'].to(device)\n",
        "\n",
        "        decoder_target = roman_seq[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio)\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, roman_tokenizer):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            urdu_seq = batch['urdu'].to(device)\n",
        "            roman_seq = batch['roman'].to(device)\n",
        "\n",
        "            decoder_target = roman_seq[:, 1:]\n",
        "\n",
        "            # Loss calculation - now properly handles padding tokens\n",
        "            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio=0.0)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n",
        "            non_pad_tokens = (decoder_target != 0).sum().item()\n",
        "\n",
        "            total_loss += loss.item() * non_pad_tokens\n",
        "            total_tokens += non_pad_tokens\n",
        "\n",
        "            # Predictions for metrics\n",
        "            pred_tokens = outputs.argmax(dim=-1)\n",
        "            for i in range(pred_tokens.size(0)):\n",
        "                predictions.append(pred_tokens[i].cpu())\n",
        "                targets.append(decoder_target[i].cpu())\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    bleu = calculate_bleu_score(predictions, targets, roman_tokenizer)\n",
        "    perplexity = calculate_perplexity(model, data_loader, criterion, device)\n",
        "    cer = calculate_cer(predictions, targets, roman_tokenizer)\n",
        "    accuracy_metrics = calculate_accuracy(predictions, targets, roman_tokenizer)\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'bleu': bleu,\n",
        "        'perplexity': perplexity,\n",
        "        'cer': cer,\n",
        "        'token_accuracy': accuracy_metrics['token_accuracy'],\n",
        "        'sequence_accuracy': accuracy_metrics['sequence_accuracy']\n",
        "    }\n",
        "\n",
        "def translate_text(model, text, urdu_tokenizer, roman_tokenizer):\n",
        "    \"\"\"Translate a single text\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    tokens = urdu_tokenizer.encode(text, out_type=int)\n",
        "    input_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        predicted_tokens = output.argmax(dim=-1).squeeze().cpu().tolist()\n",
        "\n",
        "        # Truncate prediction at first EOS token if present (EOS id = 3)\n",
        "        try:\n",
        "            eos_index = predicted_tokens.index(3)\n",
        "            predicted_tokens = predicted_tokens[:eos_index]\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "        # Remove special tokens except EOS (PAD=0, UNK=1, BOS=2)\n",
        "        clean_tokens = [t for t in predicted_tokens if t not in [0, 1, 2]]\n",
        "\n",
        "        translated_text = roman_tokenizer.decode(clean_tokens)\n",
        "\n",
        "    return translated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUuGnOwxUgGE"
      },
      "source": [
        "# Experiment RunnerThis cell defines the\n",
        "\n",
        "---\n",
        "\n",
        "`run_experiment` function to train and evaluate the model with a given configuration, including early stopping and sample translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YPOBZEcxUgGE"
      },
      "outputs": [],
      "source": [
        "def run_experiment(config, splits, urdu_tokenizer, roman_tokenizer):\n",
        "    \"\"\"Run a single experiment with given configuration\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Running experiment: {config['name']}\")\n",
        "    print(f\"Config: {config}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    test_results = None  # Initialize test_results to avoid UnboundLocalError\n",
        "\n",
        "    try:\n",
        "        # Create datasets\n",
        "        train_dataset = TranslationDataset(\n",
        "            splits['train']['urdu'], splits['train']['roman'],\n",
        "            urdu_tokenizer, roman_tokenizer\n",
        "        )\n",
        "        val_dataset = TranslationDataset(\n",
        "            splits['val']['urdu'], splits['val']['roman'],\n",
        "            urdu_tokenizer, roman_tokenizer\n",
        "        )\n",
        "        test_dataset = TranslationDataset(\n",
        "            splits['test']['urdu'], splits['test']['roman'],\n",
        "            urdu_tokenizer, roman_tokenizer\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
        "                                 shuffle=True, collate_fn=collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],\n",
        "                               shuffle=False, collate_fn=collate_fn)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'],\n",
        "                                shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        # Initialize model\n",
        "        model = Seq2SeqModel(\n",
        "            urdu_vocab_size=urdu_tokenizer.get_piece_size(),\n",
        "            roman_vocab_size=roman_tokenizer.get_piece_size(),\n",
        "            embed_dim=config['embed_dim'],\n",
        "            hidden_dim=config['hidden_dim'],\n",
        "            dropout=config['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "        # Initialize optimizer and criterion\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        # Training loop\n",
        "        best_val_bleu = 0\n",
        "        patience = 5\n",
        "        patience_counter = 0\n",
        "\n",
        "        train_losses = []\n",
        "        val_metrics = []\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            # Train\n",
        "            train_loss = train_epoch(model, train_loader, optimizer, criterion,\n",
        "                                   config.get('teacher_forcing_ratio', 0.5))\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            # Validate\n",
        "            val_results = evaluate_model(model, val_loader, criterion, roman_tokenizer)\n",
        "            val_metrics.append(val_results)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{config['epochs']}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"Val Loss: {val_results['loss']:.4f}, BLEU: {val_results['bleu']:.4f}, \"\n",
        "                  f\"Perplexity: {val_results['perplexity']:.2f}, CER: {val_results['cer']:.4f}\")\n",
        "            print(f\"Token Accuracy: {val_results['token_accuracy']:.4f}, \"\n",
        "                  f\"Sequence Accuracy: {val_results['sequence_accuracy']:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_results['bleu'] > best_val_bleu:\n",
        "                best_val_bleu = val_results['bleu']\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), f\"best_model_{config['name']}.pth\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model for testing\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(f\"best_model_{config['name']}.pth\"))\n",
        "\n",
        "            # Test evaluation\n",
        "            test_results = evaluate_model(model, test_loader, criterion, roman_tokenizer)\n",
        "\n",
        "            print(f\"\\nFinal Test Results for {config['name']}:\")\n",
        "            print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
        "            print(f\"Test BLEU: {test_results['bleu']:.4f}\")\n",
        "            print(f\"Test Perplexity: {test_results['perplexity']:.2f}\")\n",
        "            print(f\"Test CER: {test_results['cer']:.4f}\")\n",
        "            print(f\"Test Token Accuracy: {test_results['token_accuracy']:.4f}\")\n",
        "            print(f\"Test Sequence Accuracy: {test_results['sequence_accuracy']:.4f}\")\n",
        "\n",
        "            # Sample translations\n",
        "            print(f\"\\nSample Translations for {config['name']}:\")\n",
        "            sample_texts = splits['test']['urdu'][:5]\n",
        "            for i, urdu_text in enumerate(sample_texts):\n",
        "                translation = translate_text(model, urdu_text, urdu_tokenizer, roman_tokenizer)\n",
        "                actual = splits['test']['roman'][i]\n",
        "                print(f\"Urdu: {urdu_text}\")\n",
        "                print(f\"Predicted: {translation}\")\n",
        "                print(f\"Actual: {actual}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model loading or testing: {e}\")\n",
        "            # Create default test results if testing fails\n",
        "            test_results = {\n",
        "                'loss': float('inf'),\n",
        "                'bleu': 0.0,\n",
        "                'perplexity': float('inf'),\n",
        "                'cer': 1.0,\n",
        "                'token_accuracy': 0.0,\n",
        "                'sequence_accuracy': 0.0\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return default values if experiment fails completely\n",
        "        return {\n",
        "            'config': config,\n",
        "            'train_losses': [],\n",
        "            'val_metrics': [],\n",
        "            'test_results': {\n",
        "                'loss': float('inf'),\n",
        "                'bleu': 0.0,\n",
        "                'perplexity': float('inf'),\n",
        "                'cer': 1.0,\n",
        "                'token_accuracy': 0.0,\n",
        "                'sequence_accuracy': 0.0\n",
        "            },\n",
        "            'best_val_bleu': 0.0\n",
        "        }\n",
        "\n",
        "    # Ensure test_results is never None\n",
        "    if test_results is None:\n",
        "        test_results = {\n",
        "            'loss': float('inf'),\n",
        "            'bleu': 0.0,\n",
        "            'perplexity': float('inf'),\n",
        "            'cer': 1.0,\n",
        "            'token_accuracy': 0.0,\n",
        "            'sequence_accuracy': 0.0\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'train_losses': train_losses,\n",
        "        'val_metrics': val_metrics,\n",
        "        'test_results': test_results,\n",
        "        'best_val_bleu': best_val_bleu\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flhpPKOxUgGF"
      },
      "source": [
        "# Main ExecutionThis cell contains the\n",
        "\n",
        "---\n",
        "\n",
        "`main` function that orchestrates the entire experiment process, including data loading, preprocessing, tokenizer creation, and running experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkl2EN03UgGG",
        "outputId": "7313eb9c-4b2a-4dee-d963-c1363962ba7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Urdu to Roman Transliteration Experiments\n",
            "============================================================\n",
            "Loading data...\n",
            "Loading data from urdu_ghazals_rekhta dataset...\n",
            "Loaded 21003 text pairs\n",
            "Preprocessing and filtering data...\n",
            "After preprocessing: 20893 pairs\n",
            "Creating data splits...\n",
            "Data split - Train: 10446 (50.0%), Val: 5223 (25.0%), Test: 5224 (25.0%)\n",
            "Train samples: 10446\n",
            "Validation samples: 5223\n",
            "Test samples: 5224\n",
            "Creating tokenizers...\n",
            "Urdu tokenizer vocabulary size: 8000\n",
            "Roman tokenizer vocabulary size: 8000\n",
            "\n",
            "Teacher Forcing Setup\n",
            "Use teacher forcing during training? (y/n, default y): y\n",
            "Enter teacher forcing ratio [0.0-1.0] (default 0.5): 0.3\n",
            "Using teacher forcing ratio: 0.3\n",
            "\n",
            "==================================================\n",
            "Running experiment: baseline\n",
            "Config: {'name': 'baseline', 'embed_dim': 128, 'hidden_dim': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'teacher_forcing_ratio': 0.3}\n",
            "==================================================\n",
            "Epoch 1/15\n",
            "Train Loss: 5.1742\n",
            "Val Loss: 4.0274, BLEU: 0.0413, Perplexity: 56.12, CER: 0.6117\n",
            "Token Accuracy: 0.3063, Sequence Accuracy: 0.0002\n",
            "Epoch 2/15\n",
            "Train Loss: 3.3769\n",
            "Val Loss: 3.3291, BLEU: 0.1378, Perplexity: 27.91, CER: 0.5096\n",
            "Token Accuracy: 0.3762, Sequence Accuracy: 0.0172\n",
            "Epoch 3/15\n",
            "Train Loss: 2.5313\n",
            "Val Loss: 2.9186, BLEU: 0.2416, Perplexity: 18.52, CER: 0.4375\n",
            "Token Accuracy: 0.4332, Sequence Accuracy: 0.0500\n",
            "Epoch 4/15\n",
            "Train Loss: 1.9633\n",
            "Val Loss: 2.6794, BLEU: 0.3458, Perplexity: 14.58, CER: 0.3410\n",
            "Token Accuracy: 0.4919, Sequence Accuracy: 0.0892\n",
            "Epoch 5/15\n",
            "Train Loss: 1.5293\n",
            "Val Loss: 2.5975, BLEU: 0.3682, Perplexity: 13.43, CER: 0.3459\n",
            "Token Accuracy: 0.4974, Sequence Accuracy: 0.1103\n",
            "Epoch 6/15\n",
            "Train Loss: 1.2137\n",
            "Val Loss: 2.6106, BLEU: 0.3926, Perplexity: 13.61, CER: 0.3296\n",
            "Token Accuracy: 0.5096, Sequence Accuracy: 0.1036\n",
            "Epoch 7/15\n",
            "Train Loss: 0.9876\n",
            "Val Loss: 2.5689, BLEU: 0.4284, Perplexity: 13.05, CER: 0.2973\n",
            "Token Accuracy: 0.5310, Sequence Accuracy: 0.1310\n",
            "Epoch 8/15\n",
            "Train Loss: 0.8337\n",
            "Val Loss: 2.5520, BLEU: 0.4585, Perplexity: 12.83, CER: 0.2589\n",
            "Token Accuracy: 0.5534, Sequence Accuracy: 0.1662\n",
            "Epoch 9/15\n",
            "Train Loss: 0.7117\n",
            "Val Loss: 2.5783, BLEU: 0.4515, Perplexity: 13.17, CER: 0.2802\n",
            "Token Accuracy: 0.5466, Sequence Accuracy: 0.1503\n",
            "Epoch 10/15\n",
            "Train Loss: 0.6018\n",
            "Val Loss: 2.6007, BLEU: 0.4689, Perplexity: 13.47, CER: 0.2751\n",
            "Token Accuracy: 0.5527, Sequence Accuracy: 0.1627\n",
            "Epoch 11/15\n",
            "Train Loss: 0.5254\n",
            "Val Loss: 2.6161, BLEU: 0.4715, Perplexity: 13.68, CER: 0.2745\n",
            "Token Accuracy: 0.5564, Sequence Accuracy: 0.1505\n",
            "Epoch 12/15\n",
            "Train Loss: 0.4564\n",
            "Val Loss: 2.6404, BLEU: 0.4712, Perplexity: 14.02, CER: 0.3068\n",
            "Token Accuracy: 0.5496, Sequence Accuracy: 0.1474\n",
            "Epoch 13/15\n",
            "Train Loss: 0.3987\n",
            "Val Loss: 2.6667, BLEU: 0.4889, Perplexity: 14.39, CER: 0.2495\n",
            "Token Accuracy: 0.5662, Sequence Accuracy: 0.1725\n",
            "Epoch 14/15\n",
            "Train Loss: 0.3488\n",
            "Val Loss: 2.7058, BLEU: 0.5100, Perplexity: 14.97, CER: 0.2316\n",
            "Token Accuracy: 0.5816, Sequence Accuracy: 0.1859\n",
            "Epoch 15/15\n",
            "Train Loss: 0.3157\n",
            "Val Loss: 2.6651, BLEU: 0.5006, Perplexity: 14.37, CER: 0.2572\n",
            "Token Accuracy: 0.5736, Sequence Accuracy: 0.1807\n",
            "\n",
            "Final Test Results for baseline:\n",
            "Test Loss: 2.8620\n",
            "Test BLEU: 0.4922\n",
            "Test Perplexity: 17.50\n",
            "Test CER: 0.2386\n",
            "Test Token Accuracy: 0.5499\n",
            "Test Sequence Accuracy: 0.1744\n",
            "\n",
            "Sample Translations for baseline:\n",
            "Urdu: دیکھیے لب تک نہیں آتی گل عارض کی یاد\n",
            "Predicted: dekhiye lab tak nah aat gul-e- yaad k yaad\n",
            "Actual: dekhiye lab tak nah aat gul-e-riz k yaad\n",
            "--------------------------------------------------\n",
            "Urdu: آب حیواں سوں جام تجھ لب کا\n",
            "Predicted: b-e-ruhsr suu jaam tujh lab k\n",
            "Actual: b-e-haiv suu jaam tujh lab k\n",
            "--------------------------------------------------\n",
            "Urdu: کسی کا دل جو ہاتھ آیا تو دل داری سے ہاتھ آیا\n",
            "Predicted: kis k dil jo haath aay to dil-dr se haath aay\n",
            "Actual: kis k dil jo haath aay to dildr se haath aay\n",
            "--------------------------------------------------\n",
            "Urdu: سبز قندیل جل رہی ہوگی\n",
            "Predicted: sabz bay-e-p jal rah hog hog\n",
            "Actual: sabz qindl jal rah hog\n",
            "--------------------------------------------------\n",
            "Urdu: اب خواب ہے نے آرزو ارمان ہے نے جستجو یوں بھی چلو خوش ہیں مگر میں اور مری آوارگی\n",
            "Predicted: ab hvb hai ne aarz armn hai ne yuu yuu chalo chalo hai magar me aur mir mir\n",
            "Actual: ab hvb hai nai aarz armn hai nai justuj yuu bh chalo hush hai magar mai aur mir vrg\n",
            "--------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT SUMMARY\n",
            "================================================================================\n",
            "Experiment           BLEU     CER      Perplexity   Token Acc  Seq Acc   \n",
            "--------------------------------------------------------------------------------\n",
            "baseline             0.4922   0.2386   17.50        0.5499     0.1744    \n",
            "\n",
            "Best Model: baseline\n",
            "Best BLEU Score: 0.4922\n",
            "Best CER: 0.2386\n",
            "Best Token Accuracy: 0.5499\n",
            "Best Sequence Accuracy: 0.1744\n",
            "\n",
            "Experiments completed!\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run all experiments\"\"\"\n",
        "    print(\"Starting Urdu to Roman Transliteration Experiments\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading data...\")\n",
        "    processor = UrduRomanDataProcessor(\"dataset\")\n",
        "    processor.load_data()\n",
        "    processor.preprocess_data()\n",
        "\n",
        "    # Create train/val/test splits\n",
        "    print(\"Creating data splits...\")\n",
        "    splits = processor.split_data()\n",
        "\n",
        "    print(f\"Train samples: {len(splits['train']['urdu'])}\")\n",
        "    print(f\"Validation samples: {len(splits['val']['urdu'])}\")\n",
        "    print(f\"Test samples: {len(splits['test']['urdu'])}\")\n",
        "\n",
        "    # Create tokenizers\n",
        "    print(\"Creating tokenizers...\")\n",
        "    urdu_tokenizer, roman_tokenizer = create_tokenizers(\n",
        "        splits['train']['urdu'] + splits['val']['urdu'],\n",
        "        splits['train']['roman'] + splits['val']['roman']\n",
        "    )\n",
        "\n",
        "    # Interactive teacher forcing prompt\n",
        "    print(\"\\nTeacher Forcing Setup\")\n",
        "    use_tf_input = input(\"Use teacher forcing during training? (y/n, default y): \").strip().lower()\n",
        "    if use_tf_input == 'n':\n",
        "        tf_ratio = 0.0\n",
        "        print(\"Teacher forcing disabled (ratio = 0.0)\")\n",
        "    else:\n",
        "        tf_ratio_input = input(\"Enter teacher forcing ratio [0.0-1.0] (default 0.5): \").strip()\n",
        "        try:\n",
        "            tf_ratio = float(tf_ratio_input) if tf_ratio_input else 0.5\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Defaulting teacher forcing ratio to 0.5\")\n",
        "            tf_ratio = 0.5\n",
        "        tf_ratio = max(0.0, min(1.0, tf_ratio))\n",
        "        print(f\"Using teacher forcing ratio: {tf_ratio}\")\n",
        "\n",
        "    # Define experiment configurations\n",
        "    configs = [\n",
        "        {\n",
        "            'name': 'baseline',\n",
        "            'embed_dim': 128,\n",
        "            'hidden_dim': 256,\n",
        "            'dropout': 0.1,\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 15,\n",
        "            'teacher_forcing_ratio': tf_ratio\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Run experiments\n",
        "    results = []\n",
        "    for config in configs:\n",
        "        try:\n",
        "            result = run_experiment(config, splits, urdu_tokenizer, roman_tokenizer)\n",
        "            # Only add results that have both test_results and config\n",
        "            if result and 'test_results' in result and 'config' in result:\n",
        "                results.append(result)\n",
        "            else:\n",
        "                print(f\"Warning: Experiment {config['name']} returned incomplete results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error running experiment {config['name']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not results:\n",
        "        print(\"No experiments completed successfully.\")\n",
        "        return\n",
        "\n",
        "    print(f\"{'Experiment':<20} {'BLEU':<8} {'CER':<8} {'Perplexity':<12} {'Token Acc':<10} {'Seq Acc':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for result in results:\n",
        "        config = result.get('config', {})\n",
        "        test_results = result.get('test_results', {})\n",
        "\n",
        "        name = config.get('name', 'Unknown')\n",
        "        bleu = test_results.get('bleu', 0.0)\n",
        "        cer = test_results.get('cer', 1.0)\n",
        "        perplexity = test_results.get('perplexity', float('inf'))\n",
        "        token_acc = test_results.get('token_accuracy', 0.0)\n",
        "        seq_acc = test_results.get('sequence_accuracy', 0.0)\n",
        "\n",
        "        # Handle infinite perplexity for display\n",
        "        perp_str = f\"{perplexity:.2f}\" if perplexity != float('inf') else \"inf\"\n",
        "\n",
        "        print(f\"{name:<20} {bleu:<8.4f} {cer:<8.4f} {perp_str:<12} {token_acc:<10.4f} {seq_acc:<10.4f}\")\n",
        "\n",
        "    # Find best model\n",
        "    if results:\n",
        "        best_result = max(results, key=lambda x: x.get('test_results', {}).get('bleu', 0))\n",
        "        best_config = best_result.get('config', {})\n",
        "        best_test = best_result.get('test_results', {})\n",
        "\n",
        "        print(f\"\\nBest Model: {best_config.get('name', 'Unknown')}\")\n",
        "        print(f\"Best BLEU Score: {best_test.get('bleu', 0.0):.4f}\")\n",
        "        print(f\"Best CER: {best_test.get('cer', 1.0):.4f}\")\n",
        "        print(f\"Best Token Accuracy: {best_test.get('token_accuracy', 0.0):.4f}\")\n",
        "        print(f\"Best Sequence Accuracy: {best_test.get('sequence_accuracy', 0.0):.4f}\")\n",
        "\n",
        "    print(\"\\nExperiments completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the model and tokenizer files\n",
        "files.download('/content/best_model_baseline.pth')\n",
        "files.download('/content/urdu_tokenizer.model')\n",
        "files.download('/content/roman_tokenizer.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BhCKQ_XcdFh7",
        "outputId": "c0a51380-ff97-4ed4-ec4f-f218d99026a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9001b654-0cee-4fc1-8134-91d6315708d3\", \"best_model_baseline.pth\", 54112416)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ee0b85f-673b-4785-ad3d-408bd222a564\", \"urdu_tokenizer.model\", 395643)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79ca1bed-4d85-402d-817e-1cb55e84518b\", \"roman_tokenizer.model\", 361449)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}